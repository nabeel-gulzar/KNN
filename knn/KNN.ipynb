{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from math import sqrt, floor\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "from collections import Counter\n",
    "from itertools import dropwhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    punctuations = string.punctuation\n",
    "    translations = text.maketrans(punctuations, ''.join([' ']*len(punctuations)))\n",
    "    return text.translate(translations)\n",
    "\n",
    "def remove_numerics(string):\n",
    "    return re.sub(r'\\d+', '', string)\n",
    "\n",
    "def remove_hashtags(string):\n",
    "    return re.sub(r'#\\w+', '', string)\n",
    "\n",
    "def remove_handles(string):\n",
    "    return re.sub(r'@\\w+', '', string)\n",
    "\n",
    "url_re = r'(http:\\/\\/)?t\\.co\\/([a-zA-Z0-9])+'\n",
    "def remove_urls(text):\n",
    "    return re.sub(url_re, '', text)\n",
    "\n",
    "def remove_stopwords(string, stopwords):\n",
    "    return ' '.join(['' if word in stopwords else word for word in string.split()])\n",
    "\n",
    "def is_stopword(word):\n",
    "    return word in stop_words\n",
    "\n",
    "def is_hashtag(word):\n",
    "    return re.match(r'#\\w+', word)\n",
    "\n",
    "def is_handle(word):\n",
    "    return re.match(r'@\\w+', word)\n",
    "\n",
    "def is_numeric(word):\n",
    "    return re.match(r'^\\d+$', word)\n",
    "\n",
    "def get_valid_word(word):\n",
    "    word = word.lower()\n",
    "    sw = is_stopword(word)\n",
    "    ht = is_hashtag(word)\n",
    "    th = is_handle(word)\n",
    "    if sw or ht or th:\n",
    "        return ''\n",
    "    word = remove_punctuations(word)\n",
    "    return '' if is_numeric(word) else word\n",
    "    \n",
    "    return word\n",
    "\n",
    "# param: tweets string[], args = {}\n",
    "def create_bow(tweets, args):\n",
    "    bow = []\n",
    "    text = ' '.join(tweets)\n",
    "    text = text.lower()\n",
    "    if len(args['stopwords']):\n",
    "        print('removing stop words...')\n",
    "        text = remove_stopwords(text, args['stopwords'])\n",
    "    if args['remove_urls']:\n",
    "        print('removing urls...')\n",
    "        text = remove_urls(text)\n",
    "    if args['remove_handles']:\n",
    "        print('removing handles...')\n",
    "        text = remove_handles(text)\n",
    "    if args['remove_hashtags']:\n",
    "        print('removing hashtags...')\n",
    "        text = remove_hashtags(text)\n",
    "    if args['remove_punctuations']:\n",
    "        print('removing punctuations...')\n",
    "        text = remove_punctuations(text)\n",
    "    if args['remove_numerics']:\n",
    "        print('removing numerics...')\n",
    "        text = remove_numerics(text)\n",
    "    text = text.split()\n",
    "    bow = Counter(text)\n",
    "    most_common = args.get('most_common', 0)\n",
    "    if args['remove_singles']:\n",
    "        for k, c in dropwhile(lambda item: item[1]>1, bow.most_common()):\n",
    "            del bow[k]\n",
    "    if most_common:\n",
    "        print('returning most common {}'.format(most_common))\n",
    "        return bow.most_common(most_common)\n",
    "    return bow\n",
    "\n",
    "def tokenize(text, args):\n",
    "    text = text.lower()\n",
    "    if len(args['stopwords']):\n",
    "        text = remove_stopwords(text, args['stopwords'])\n",
    "    if args['remove_urls']:\n",
    "        text = remove_urls(text)\n",
    "    if args['remove_handles']:\n",
    "        text = remove_handles(text)\n",
    "    if args['remove_hashtags']:\n",
    "        text = remove_hashtags(text)\n",
    "    if args['remove_punctuations']:\n",
    "        text = remove_punctuations(text)\n",
    "    if args['remove_numerics']:\n",
    "        text = remove_numerics(text)\n",
    "    words = text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = { 'positive': 1, 'negative': -1, 'neutral': 0 }\n",
    "labels = [-1, 0, 1]\n",
    "def process_data(bow, tweets, labels, options):\n",
    "    vocab_size = len(bow)\n",
    "    sample_size = len(tweets)\n",
    "    vocab = list(bow)\n",
    "    data_x = np.zeros((sample_size, vocab_size))\n",
    "    for i, sample in enumerate(tweets):\n",
    "        for word in tokenize(sample, options):\n",
    "            try:\n",
    "                word_index = vocab.index(word)\n",
    "                data_x[i][word_index] += 1\n",
    "            except:\n",
    "                pass\n",
    "    return data_x, [Label[l] for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing stop words...\n",
      "removing urls...\n",
      "removing handles...\n",
      "removing punctuations...\n",
      "removing numerics...\n",
      "Time taken to create BOW 2.659074306488037\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train/train.csv')\n",
    "test_df = pd.read_csv('data/test/test.csv')\n",
    "\n",
    "stopwprds = []\n",
    "with open('data/stop_words.txt', 'r') as fd:\n",
    "    stopwords = fd.read().split('\\n')\n",
    "\n",
    "bow_options = {'stopwords': stopwords,\n",
    "               'remove_urls': True,\n",
    "               'remove_handles': True,\n",
    "               'remove_hashtags': False,\n",
    "               'remove_punctuations': True,\n",
    "               'remove_numerics': True,\n",
    "               'remove_singles': False,\n",
    "              }\n",
    "\n",
    "bow = create_bow(train_df['Tweet'], bow_options)\n",
    "\n",
    "starttime = time.time()\n",
    "training_x, training_y = process_data(bow, train_df['Tweet'], train_df['Sentiment'], bow_options)\n",
    "test_x, test_y = process_data(bow, test_df['Tweet'], test_df['Sentiment'], bow_options)\n",
    "endtime = time.time()\n",
    "print('Time taken to create BOW', endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_euclidean(v1, v2):\n",
    "    raw_distance = v1-v2\n",
    "    squared_distance = np.dot(raw_distance, raw_distance)\n",
    "    d = sqrt(squared_distance)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing distance matrix...\n",
      "distance matrix computed\n",
      "time taken to compute distance matrix 988.8986740112305\n"
     ]
    }
   ],
   "source": [
    "def compute_distance_matrix(training_x, test_x):\n",
    "    return np.array([[compute_euclidean(ts, tr) for tr in training_x] for ts in test_x])\n",
    "\n",
    "print('computing distance matrix...')\n",
    "starttime = time.time()\n",
    "distance_matrix = compute_distance_matrix(training_x, test_x)\n",
    "endtime = time.time()\n",
    "print('distance matrix computed')\n",
    "print('time taken to compute distance matrix', endtime - starttime)\n",
    "#time taken to compute distance matrix 271.697865486145 [with multiply sum] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting prediction...\n",
      "completed prediction\n",
      "Time taken to compute predictions 30.336062908172607\n"
     ]
    }
   ],
   "source": [
    "def partition(arr, l, r):       \n",
    "    x = arr[r] \n",
    "    i = l \n",
    "    for j in range(l, r): \n",
    "        if arr[j] <= x: \n",
    "            arr[i], arr[j] = arr[j], arr[i] \n",
    "            i += 1              \n",
    "    arr[i], arr[r] = arr[r], arr[i] \n",
    "    return i \n",
    "\n",
    "def quick_select(arr, l, r, k): \n",
    "    if (k > 0 and k <= r - l + 1): \n",
    "        index = partition(arr, l, r) \n",
    "        if (index - l == k - 1): \n",
    "            return arr[index] \n",
    "        if (index - l > k - 1): \n",
    "            return quick_select(arr, l, index - 1, k) \n",
    "        return quick_select(arr, index + 1, r, k - index + l - 1) \n",
    "    return INT_MAX\n",
    "\n",
    "def get_kth_smallest(arr, k):\n",
    "    n = len(arr) \n",
    "    return quick_select(arr, 0, n - 1, k)\n",
    "\n",
    "def get_k_nearest_indexes(distance_matrix, k):\n",
    "    temp_distance = np.array(distance_matrix)\n",
    "    kth_smallest = get_kth_smallest(temp_distance, k)\n",
    "    for i, d in enumerate(distance_matrix):\n",
    "        if d <= kth_smallest:\n",
    "            yield i\n",
    "\n",
    "def get_mod(arr):\n",
    "    label_counter = Counter(arr)\n",
    "    max_count = label_counter.most_common(1)[0][1]\n",
    "    return [k for k, c in label_counter.items() if c == max_count]\n",
    "    \n",
    "def assign_label(distance_matrix, training_labels, k):\n",
    "    k_nearest_labels = [training_labels[i] for i in get_k_nearest_indexes(distance_matrix, k)]\n",
    "    mod = get_mod(k_nearest_labels)\n",
    "    if len(mod) > 1:\n",
    "        if k == 1:\n",
    "            return mod[randint(0, len(mod) - 1)]\n",
    "        return assign_label(distance_matrix, training_labels, k-1)\n",
    "    return mod[0]\n",
    "\n",
    "def compute_accuracy(gold, predicted):\n",
    "    if len(gold) != len(predicted):\n",
    "        print('label arrays should have same size')\n",
    "        return\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(gold)):\n",
    "        if gold[i] == predicted[i]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    return correct/(correct + incorrect)\n",
    "\n",
    "def compute_performance(gold, prediction):\n",
    "    accuracy = compute_accuracy(gold, prediction)\n",
    "    return accuracy\n",
    "\n",
    "def predict(distance_matrix, training_y, k):\n",
    "    test_size = distance_matrix.shape[0]\n",
    "    return [assign_label(distance_matrix[test_i], training_y, k) for test_i in range(test_size)]\n",
    "\n",
    "def generate_confusion_matrix(gold, prediction, labels):\n",
    "    if len(gold) != len(prediction):\n",
    "        print('label arrays should have same size')\n",
    "        return\n",
    "    label_count = len(labels)\n",
    "    cm = np.zeros((label_count, label_count))\n",
    "    for i in range(len(prediction)):\n",
    "        cm[labels.index(prediction[i])][labels.index(gold[i])] += 1\n",
    "    return cm\n",
    "\n",
    "print('starting prediction...')\n",
    "starttime = time.time()\n",
    "#for k in [3]:\n",
    "prediction = predict(distance_matrix, training_y, 2)\n",
    "endtime = time.time()\n",
    "print('completed prediction')\n",
    "print('Time taken to compute predictions', endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_macro_average(prediction, gold, labels):\n",
    "    label_count = len(labels)\n",
    "    cm = generate_confusion_matrix(test_y, prediction, labels)\n",
    "    print(cm)\n",
    "    print('-------------')\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    for i in range(label_count):\n",
    "        negatives = [n for n in range(label_count) if n != i]\n",
    "        tp = cm[i][i]\n",
    "        fn = np.array([cm[j][i] for j in negatives]).sum()\n",
    "        fp = np.array([cm[i][j] for j in negatives]).sum()\n",
    "        tn = np.array([np.array([cm[j][k] for j in negatives]).sum() for k in negatives]).sum()\n",
    "        print('{}\\t{}'.format(tp, fp))\n",
    "        print('{}\\t{}'.format(fn, tn))\n",
    "        print('-------({})--------'.format(labels[i]))\n",
    "\n",
    "        precision += (tp/(tp+fp))\n",
    "        recall += (tp/(tp+fn))\n",
    "    macro_avg_precision = precision/label_count\n",
    "    macro_avg_recall = recall/label_count\n",
    "    f1_score = (2*macro_avg_precision*macro_avg_recall)/(macro_avg_precision+macro_avg_recall)\n",
    "    return {'f1': f1_score, 'precision': macro_avg_precision, 'recall': macro_avg_recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[799.  89.  46.]\n",
      " [861. 438. 135.]\n",
      " [174.  88. 291.]]\n",
      "-------------\n",
      "799.0\t135.0\n",
      "1035.0\t952.0\n",
      "-------(-1)--------\n",
      "438.0\t996.0\n",
      "177.0\t1310.0\n",
      "-------(0)--------\n",
      "291.0\t262.0\n",
      "181.0\t2187.0\n",
      "-------(1)--------\n",
      "F1 Score: 0.5750\n",
      "Precision: 0.5624\n",
      "Recall: 0.5881\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "macro = compute_macro_average(prediction, test_y, [-1, 0, 1])\n",
    "print('F1 Score: %.4f' % macro['f1'])\n",
    "print('Precision: %.4f' % macro['precision'])\n",
    "print('Recall: %.4f' % macro['recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10060\n"
     ]
    }
   ],
   "source": [
    "print(len(training_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed everything\n",
    "#starting prediction...\n",
    "#Accuracy for k=1 is 0.5234508729887025\n",
    "#Accuracy for k=3 is 0.47107155083875385\n",
    "#Accuracy for k=5 is 0.4577199589181787\n",
    "#Accuracy for k=7 is 0.4471071550838754\n",
    "#Accuracy for k=10 is 0.43957548784662787\n",
    "#completed prediction\n",
    "#Time taken to compute predictions 166.01523804664612"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
